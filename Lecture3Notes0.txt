Command line args are arguments to the main function which runs when the program runs

Name of the program shows as the first argc argument

What are the concepts of algorithms we are talking about
  there is a given input, run through an algorithm which returns an output

  [INPUT] -------> [ALGORITHM] --------> [OUTPUT]

  We specify preconditions
    a condition that must be true for an algorithm to run
      x must be of type int etc...
      factorial(n): n is input, precondition n>0

  We specify post conditions
    expected changes, or return value after algorithm execution
      array A = [90, 23, 54], pass in a and have it sorted
        post condition is A being sorted

  Correctness: we need a correct answer, our algorithm has to do what we want it to do

  Cost: Cost only counts if correctness is satisfied
    What is the memory use and runtime of the algorithm
    Runtime: Too many variables to measure a 'pure' runtime (OS, Language, Implementation, Computer)
      The best compromise is counting code lines; this gives a theoretical runtime
        Suppose each line has 'cost' equal to 1. This gives an estimate of runtime, proportional to actual runtime
      Big-Oh notation: Upper bound on growth of algorithm runtime as n->inf
      Growth rate: How does the algorithm scale with the input size
        Function f(n) = n
                 f(n) = n^2
                 f(n) = n^3
                 f(n) = 2^n

      for x = 0 to A.rows
        for y = 0 to A.cols
          if A[x][y]==v
            index X = x
            index Y = y
            break
      return(index x, index y)

        WORST CASE: n^2
      Relate the precise runtime to a growth function. Define performance by that growth.
